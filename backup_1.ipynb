{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f72776",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO (You Only Look Once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2683e72",
   "metadata": {},
   "source": [
    "Matheus Schmitz  \n",
    "<a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a>  \n",
    "<a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051a946",
   "metadata": {},
   "source": [
    "**Configuration File**: https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25bb2ff",
   "metadata": {},
   "source": [
    "## Part 1 - Neural Network Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd477bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910fa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the configuration file\n",
    "# Returns the blocks used to build the neural network\n",
    "def parse_config_file(config_file):\n",
    "    \n",
    "    # Open the file for reading\n",
    "    file = open(config_file, 'r')\n",
    "    \n",
    "    # Read the lines and convert to a list\n",
    "    # Remove blank lines\n",
    "    # Remove comments\n",
    "    # Remove white spaces\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [line for line in lines if len(line) > 0]\n",
    "    lines = [line for line in lines if line[0] != '#']\n",
    "    lines = [line.strip() for line in lines]\n",
    "    \n",
    "    # Dictionary and list of hyperparameter blocks\n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    # Loop through the lines\n",
    "    for line in lines:\n",
    "        \n",
    "        # Get the type (class) of the block of hyperparameters\n",
    "        if line[0] == '[':\n",
    "            # We are starting a new block, so first add the previous/current one to the list of blocks, then start the new one\n",
    "            if len(block) != 0:\n",
    "                blocks.append(block)\n",
    "                block = {}\n",
    "            # Name the block type accordingly (minus the [] brackets)\n",
    "            block['type'] = line[1:-1].strip()\n",
    "        # If/while inside a block, get the hyperparameter and value to use\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            block[key.strip()] = value.strip()\n",
    "    \n",
    "    # Need an extra line to append the last block (because usually we append the block before starting the next one)\n",
    "    blocks.append(block)\n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54aecd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the network modules\n",
    "# Returns PyTorch objects\n",
    "def create_modules(blocks):\n",
    "        \n",
    "    # Info about the neural network input parameters\n",
    "    net_info = blocks[0]\n",
    "    \n",
    "    # Create an object to build the modules\n",
    "    # https://pytorch.org/docs/master/generated/torch.nn.ModuleList.html\n",
    "    module_list = nn.ModuleList()\n",
    "    \n",
    "    # Number of color channels in the images\n",
    "    num_filters = 3\n",
    "    \n",
    "    # List used in the route layers to keep record of all filters' outputs\n",
    "    output_filters = []\n",
    "    \n",
    "    # Iterate through the blocks and create the neural network's modules (layers)\n",
    "    for index, block in enumerate(blocks[1:]):\n",
    "        \n",
    "        # Create the current module (sequence of steps)\n",
    "        # https://pytorch.org/docs/master/generated/torch.nn.Sequential.html\n",
    "        module = nn.Sequential()\n",
    "        \n",
    "        # Check the module type\n",
    "        if block['type'] == 'convolutional':\n",
    "            \n",
    "            # Extract the hyperparameters for a convolutional layer\n",
    "            activation = block['activation']\n",
    "            filters = int(block['filters'])\n",
    "            padding = int(block['pad'])\n",
    "            kernel_size = int(block['size'])\n",
    "            stride = int(block['stride'])\n",
    "            bias = 0\n",
    "            \n",
    "            # Add batch normalization if the layer has it\n",
    "            try:\n",
    "                batch_normalize = int(block['batch_normalize'])\n",
    "            except:\n",
    "                batch_normalize = 0    \n",
    "                \n",
    "            # Adjust the padding\n",
    "            if padding:\n",
    "                pad = (kernel_size - 1)//2\n",
    "            else:\n",
    "                pad = 0\n",
    "                \n",
    "            # Create the convolutional layer\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html\n",
    "            conv = nn.Conv2d(in_channels = num_filters,\n",
    "                             out_channels = filters,\n",
    "                             kernel_size = kernel_size,\n",
    "                             stride = stride,\n",
    "                             padding = pad,\n",
    "                             bias = bias)\n",
    "            \n",
    "            # Add the convolutional layer to the list of modules\n",
    "            module.add_module(f'conv_{index}', conv)\n",
    "            \n",
    "            # Add the batch normalization layer\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(num_features = filters)\n",
    "                module.add_module(f'batch_norm_{index}', bn)\n",
    "                \n",
    "            # Check the activation type and add an activation layer (YOLO v3 uses only LeakyReLU activations)\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html\n",
    "            if activation == 'leaky':\n",
    "                activn = nn.LeakyReLU(negative_slope = 0.1,\n",
    "                                      inplace = True)\n",
    "                module.add_module(f'leaky_{idx}', activn)\n",
    "            \n",
    "        # Upsampling layer - used to restore the image resolution to the size of the previous layer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html   \n",
    "        elif block['type'] == 'upsample':\n",
    "            stride = int(block['stride'])\n",
    "            upsample = nn.Upsample(scale_factor = 2,\n",
    "                                   mode = 'bilinear')\n",
    "            module.add_module(f'upsample_{index}', upsample)\n",
    "            \n",
    "        # Route layer - used to calculate the output depth (filters) resulting from concatenation. Similar to concat layers.\n",
    "        # When the attribute has a single value, the layer generates feature maps indexed by that value.\n",
    "        # E.g. if route = -4, then the route layer will have the feature maps of a layer 4 steps behind it.\n",
    "        # When the attribute has two values, the layer returns a concatenation of the feature maps of both layers by index number.\n",
    "        # E.g. if route = -1, 61, the layer will have the feature maps of the previous layer (-1) and the 61st layer, ...\n",
    "        # ... with both feature maps concatenated along the depth dimension.\n",
    "        # https://github.com/pjreddie/darknet/issues/545\n",
    "        # https://github.com/AlexeyAB/darknet/issues/487#issuecomment-374902735\n",
    "        # https://github.com/AlexeyAB/darknet/issues/279#issuecomment-397248821\n",
    "        # https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects\n",
    "        elif block['type'] == 'route':\n",
    "            block['layers'] == block['layers'].split(',')\n",
    "            \n",
    "            # Route \"start\"\n",
    "            start = int(block['layers'][0])\n",
    "            \n",
    "            # Check if there are two attributes/values\n",
    "            # If yes, set the second attribute as the route \"end\"\n",
    "            try:\n",
    "                end = int(block['layers'][1])\n",
    "            # If not, set the end to zero\n",
    "            except:\n",
    "                end = 0\n",
    "            \n",
    "            # Calculate relative positions\n",
    "            if start > 0:\n",
    "                start = start - index\n",
    "            if end > 0:\n",
    "                end = end = index\n",
    "                \n",
    "            # Create the layer\n",
    "            route = EmptyLayer()\n",
    "            \n",
    "            # Add the layer to the neural network module\n",
    "            module.add_module(f'route_{index}', route)\n",
    "            \n",
    "            # Extract the filters\n",
    "            if end < 0:\n",
    "                filters = output_filters[index + start] + output_filters[index + end]\n",
    "            else:\n",
    "                filters = output_filters[index + start]\n",
    "                \n",
    "        # Shortcut layer - same as a skip layer in ResNet.\n",
    "        # E.g. if the hyperparameter is -3, then the shortcut layer's output is obtained by merging the feature vectors ...\n",
    "        # ... from the previous layer abd the layer 3 steps behind the shortcut layer.\n",
    "        elif block['type'] == 'shortcut':\n",
    "                \n",
    "            # Create the layer\n",
    "            shortcut = EmptyLayer()\n",
    "            \n",
    "            # Add layer to the model\n",
    "            module.add_module(f'shortcut_{index}', shortcut)\n",
    "        \n",
    "        # YOLO layer with anchor detection\n",
    "        # The YOLO layer is the detection layer. The anchors describe 9 total anchors, yet only those anchors indexed by ...\n",
    "        # ... the mask attributes are used. E.g. if the mask value is 0, 1, 2, that means the first, second and third anchors ...\n",
    "        # ... will be used. This makes sense given that each cell in the detection layer predicts 3 boxes. In total, we have ...\n",
    "        # ... detection in three scales, resulting in 9 anchors.\n",
    "        # Anchors: Predetermined set of bounding boxes with specific height-width ratios.\n",
    "        # Mask: List of anchor IDs which the layer is responsible for predictign.\n",
    "        # Num: total number of anchors.\n",
    "        # YOLO v3 predicts a predetermined set of anchors, which have initial sizes (height, width), some of which (the one ...\n",
    "        # ... closest to the object size) will be redimensioned to the object's size.\n",
    "        # Each YOLO layer must know all anchors, but is responsible for only a subset of them.\n",
    "        # The mask tells the layer which anchors it should use for predicting. The first YOLO layer is assigned anchors 6,7,8, ...\n",
    "        # ... the second gets 3,4,5, and the third gets 0,1,2.\n",
    "        elif block['type'] == 'yolo':\n",
    "            \n",
    "            # Extract the mask values\n",
    "            mask = list(map(int, block['mask'].split(',')))\n",
    "            \n",
    "            # Extract the anchor values\n",
    "            anchors = list(map(int, block['anchors'].split(',')))\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors), 2)]\n",
    "            \n",
    "            # Filter the list of anchors using the mask\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "            \n",
    "            # Create the anchor detection layer\n",
    "            detection = DetectAnchors(anchors)\n",
    "            \n",
    "            # Add layer to the model\n",
    "            module.add_module(f'detection_{index}', detection)\n",
    "            \n",
    "        # Load the list of modules (layer groups), filters and output filters\n",
    "        module_list.append(module)\n",
    "        num_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)\n",
    "\n",
    "# Function summary:\n",
    "# YOLO has 5 layer types: Convolutional, Upsample, Route, Shortcut and YOLO.\n",
    "# All customization of a YOLO model is done by adjusting hyperparameter values in the configuration file.\n",
    "# The configuration file describes the YOLO network layout block by block.\n",
    "# The YOLO architecture is also known as Darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d2ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "# Take a feature vector of detections and transform it into a 2D tensor, in which each tensor line corresponds to ...\n",
    "# ... the attributes of one bounding box (anchor).\n",
    "# Arguments:\n",
    "# prediction (tensor): previous output\n",
    "# input_dim (int): dimension of the input image\n",
    "# anchros (list(tuple)): anchors used in the YOLO detection layer\n",
    "# num_classes (int): total number of classes\n",
    "# CUDA (bool): optional argument to define whether or not to use GPU\n",
    "# Function retuns:\n",
    "# prediction (tensor): redimensioned (3D tensor) prediction output of the current YOLO layer.\n",
    "# The three dimensions are: [batch size, number of bounding boxes, bound box attributes]\n",
    "def make_predictions(prediction, input_dim, anchors, num_classes, CUDA = True):\n",
    "    \n",
    "    # Hyperparameters for the predictions\n",
    "    batch_size = prediction.size(0)\n",
    "    stride = input_dim // prediction.size(2)\n",
    "    grid_size = input_dim // stride\n",
    "    bbox_attribuites = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    # Adjust the prediction object's shape\n",
    "    prediction = prediction.view(batch_size, bbox_attribuites * num_anchors, grid_size * grid_size)\n",
    "    \n",
    "    # Transpose the matrix\n",
    "    prediction = prediction.transpose(1, 2).contiguous()\n",
    "    \n",
    "    # New shape adjust\n",
    "    prediction = prediction.view(batch_size, grid_size * grid_size * num_anchors, bbox_attribuites)\n",
    "    \n",
    "    # Proportionally resize the anchors based on stride\n",
    "    # [(,),(,),(,)] -> tensor([[,],[,],[,]])   size([3,2])\n",
    "    anchors = [(anchor[0]/stride, anchor[1]/stride) for anchor in anchors]\n",
    "    \n",
    "    # Sigmoid transformation: centre_X, centre_Y, objectness score\n",
    "    prediction[:, :, 0] = torch.sigmoid(prediction[:, :, 0])\n",
    "    prediction[:, :, 1] = torch.sigmoid(prediction[:, :, 1])\n",
    "    prediction[:, :, 4] = torch.sigmoid(prediction[:, :, 4])\n",
    "    \n",
    "    # Add the grids for the coordinate centers\n",
    "    grid = np.arange(grid_size)\n",
    "    a, b = np.meshgrid(grid, grid)\n",
    "    \n",
    "    # Adjust the shapes\n",
    "    x_offset = torch.FloatTensor(a).view(-1, 1)\n",
    "    y_offset = torch.FloatTensor(b).view(-1, 1)\n",
    "    \n",
    "    # Verify whether to use GPU\n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "        prediction = prediction.cuda()\n",
    "        \n",
    "    # Concatenate x and y for the prediction\n",
    "    x_y_offset = torch.cat((x_offset, y_offset), dim=1).repeat(1, num_anchors).view(-1, 2).unsqueeze(0)\n",
    "    prediction[:, :, :2] += x_y_offset\n",
    "    \n",
    "    # Convert the object with the anchor values to a float tensor\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "    \n",
    "    # If using GPU, send the anchors to the GPU too\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "        \n",
    "    # Matrix operations for the final anchor value\n",
    "    anchors = anchors.repeat(grid_size * grid_size, 1).unsqueeze(0)\n",
    "    \n",
    "    # Element-wise multiplication of the anchors' predictions\n",
    "    prediction[:, :, 2:4] = torch.exp(prediction[:, :, 2:4]) * anchors\n",
    "    \n",
    "    # Sigmoid activation for the class scores\n",
    "    prediction[:, :, 5:5 + num_classes] = torch.sigmoid(prediction[:, :, 5:5 + num_classes])\n",
    "    \n",
    "    # Reshape the predictions map to the size of the input image\n",
    "    prediction[:, :, :4] *= stride\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc02db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that returns the route (concat) layer of the neural network\n",
    "# Used to concatenate withthe network body, simplifying changes to the input data\n",
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d3fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor detection class\n",
    "class DetectAnchors(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectAnchors, self).__init__()\n",
    "        self.anchors = anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68bbf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO architecture, also referred to as Darknet in the paper's documentation\n",
    "# Has the convolutional layers\n",
    "class Darknet(nn.Module):\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self, config_file):\n",
    "        \n",
    "        # Initialize the class\n",
    "        super(Darknet, self).__init__()\n",
    "        \n",
    "        # Read the configuration file\n",
    "        self.blocks = parse_config_file(config_file)\n",
    "        \n",
    "        # Create the meural networks' modules\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    # Forward propagation\n",
    "    def forward(self, x, CUDA):\n",
    "        \n",
    "        # Modules (network's layers)\n",
    "        modules = self.blocks[1:]\n",
    "        \n",
    "        # Cache of all layer outputs, needed for the route/shortcut layers\n",
    "        outputs = {}\n",
    "        \n",
    "        # Track if the first detetion layer was found\n",
    "        write = False\n",
    "        \n",
    "        # Loop through the modules\n",
    "        for idx, module in enumerate(modules):\n",
    "            \n",
    "            # Module type\n",
    "            module_type = (module['type'])\n",
    "            \n",
    "            # Convolution and upsample layers\n",
    "            if module_type == 'convolution' or module_type == 'upsample':\n",
    "                \n",
    "                # Define the layer\n",
    "                x = self.module_list[i](x)\n",
    "            \n",
    "            # Route layers: concatenate two feature maps from other layers\n",
    "            elif module_type == 'route':\n",
    "                \n",
    "                # Layers\n",
    "                layers = [int(a) for a in module['layers']]\n",
    "                \n",
    "                if layers[0] > 0:\n",
    "                    layers[0] = layers[0] - idx\n",
    "                    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[idx + (layers[0])]\n",
    "                else:\n",
    "                    if layers[1] > 0:\n",
    "                        layers[1] = layers[1] - idx\n",
    "                        \n",
    "                    feature_map_1 = outputs[idx + layers[0]]\n",
    "                    feature_map_2 = outputs[idx + layers[1]]\n",
    "                    \n",
    "                    # Concatenate along the depth dimension\n",
    "                    x = torch.cat((feature_map_1, feature_map_), dim = 1)\n",
    "            \n",
    "            # Shortcut layer\n",
    "            elif module_type == 'shortcut':\n",
    "                \n",
    "                # Origin layer\n",
    "                from_layer = int(module['from'])\n",
    "\n",
    "                # Addition\n",
    "                x = outputs[idx - 1] + outputs[idx + from_layer]\n",
    "                \n",
    "            # YOLO layer\n",
    "            elif module == 'yolo':\n",
    "                \n",
    "                # Neural network's hyperparameters\n",
    "                anchors = self.module_list[idx][0].anchors\n",
    "                input_dim = int(self.net_info['height'])\n",
    "                num_classes = int(module['classes'])\n",
    "                \n",
    "                # Make predictions (aka detect objects in the image)\n",
    "                x = make_predictions(prediction = x,\n",
    "                                     input_dim = input_dim,\n",
    "                                     anchors = anchors,\n",
    "                                     num_classes = num_classes,\n",
    "                                     CUDA = CUDA)\n",
    "                \n",
    "                # If this is the first detection layer, then x represents the detections\n",
    "                if write is False:\n",
    "                    detections = x\n",
    "                    write = True\n",
    "                # If this is not the first detection layer, concatenate the predictions with the ones from previous layers\n",
    "                else:\n",
    "                    detections = torch.cat((detection, x), dim = 1)\n",
    "                    \n",
    "            outputs[idx] = x\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    \n",
    "    # Backward propagation\n",
    "    # Loading weights from the pre-trained model\n",
    "    def load_weights(self, weights_file):\n",
    "        \n",
    "        # Open the weights file\n",
    "        wf = open(weight_file, 'rb')\n",
    "        \n",
    "        # Extract the header\n",
    "        header = np.fromfile(wf, dtype=np.int32, count=5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]\n",
    "        \n",
    "        # Load the weights\n",
    "        weights = np.fromfile(wf, dtype=np.float32)\n",
    "        \n",
    "        # Control parameter to track where in the weights file we are\n",
    "        prt = 0\n",
    "        \n",
    "        # Loop through the module types\n",
    "        for idx in range(len(self.module_list)):\n",
    "            \n",
    "            # Extract the module type\n",
    "            module_type = self.blocks[idx+1]['type']\n",
    "            \n",
    "            # Load the weights for the convolutional layers\n",
    "            if module_type == 'convolutional':\n",
    "                model = self.module_list[idx]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[idx+1]['batch_normalize'])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "                conv = model[0]\n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    \n",
    "                    # Load the weights for the batch normalization layer\n",
    "                    bn = model[1]\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "                    bn_biases = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    bn_weights = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    # Adjust dimensions\n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_biases.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_biases.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_biases.view_as(bn.running_var)\n",
    "                    \n",
    "                    # Copy the data to the model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    num_biases = conv.bias.numel()\n",
    "                    \n",
    "                    # load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "                    \n",
    "                    # Reshape the loaded weights accorindg to the model weights' dimension\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                    \n",
    "                    # Copy the data to the model\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                # Load the convolutional layer's weights (same with or without batch_normalization)\n",
    "                num_weights = conv.weight.numel()\n",
    "                conv_weights = torch.from_numpy(weights[ptr : ptr+num_bn_biases])\n",
    "                ptr += num_bn_biases\n",
    "                \n",
    "                # Reshape the loaded weights accorindg to the model weights' dimension\n",
    "                conv_weights = conv_weights.view_as(conv.bias.data)\n",
    "\n",
    "                # Copy the data to the model\n",
    "                conv.weight.data.copy_(conv_weights)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4042a49",
   "metadata": {},
   "source": [
    "## Part 2 - Input Processor Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756a2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Py Data Stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# System manipulation\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse command line arguments\n",
    "def arg_parse():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Object Detector with YOLO')\n",
    "    parse.add_argument(\"--input\", dest=\"input\", help=\"Directory with images/videos for object detection.\", default=\"input\", type=str)\n",
    "    parse.add_argument(\"--output\", dest=\"output\", help=\"Directory to store images/videos with detected objects.\", default=\"output\", type=str)\n",
    "    parse.add_argument(\"--batch\", dest=\"batch\", help=\"Batch size.\", default=1)\n",
    "    parse.add_argument(\"--confidence\", dest=\"confidence\", help=\"Confidense threshold to filter predictions.\", default=0.7)\n",
    "    parse.add_argument(\"--nms_thresh\", dest=\"nms_thresh\", help=\"NMS threshold.\", default=0.4)\n",
    "    parse.add_argument(\"--cfg_file\", dest=\"cfg_file\", help=\"YOLO configuration file.\", default=\"config/yolov3.cfg\", type=str)\n",
    "    parse.add_argument(\"--weights\", dest=\"weights\", help=\"File with pretrained weights.\", default=\"weights/yolov3.weights\", type=str)\n",
    "    parse.add_argument(\"--resolution\", dest=\"resolution\", help=\"Resolution of the input images. Increase to improve accuracy. Decrease to speed up detection.\", default=\"384\", type=str)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9df834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw the bounding box\n",
    "def draw_bbox(x, results, classes, colors):\n",
    "    \n",
    "    # Define the box's corners\n",
    "    corner1 = tuple(x[1:3].int())\n",
    "    corner2 = tuple(x[3:5].int())\n",
    "    \n",
    "    # Image\n",
    "    img = results[int(x[0])]\n",
    "    \n",
    "    # Line thickness\n",
    "    line_thickness = round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    cls = int(x[-1])\n",
    "    \n",
    "    # Color\n",
    "    color = random.choice(colors)\n",
    "    \n",
    "    # Label\n",
    "    label = f'{classes[cls]}'\n",
    "    \n",
    "    # Define the bounding box's limits\n",
    "    cvs.rectangle(img, corner1, corner2, color, thickness=line_thickness)\n",
    "    \n",
    "    # Font thickness\n",
    "    font_thickness = max(tl -1, 1)\n",
    "    \n",
    "    # Label text size\n",
    "    font_size = cvs.getTextSize(label, 0, fontScale=font_thickness/3, thickness=font_thickness)[0]\n",
    "    \n",
    "    # Bbox corners\n",
    "    corner2 = corner1[0] + font_size[0] + 3, corner1[1] - font_size[1] - 3\n",
    "    \n",
    "    # Draw the bbox and incluse the class label\n",
    "    cv2.rectangle(img, corner1, corner2, color, -1)\n",
    "    cv2.putText(img, \n",
    "                label, \n",
    "                (corner1[0], corner1[1] - 2), \n",
    "                0, \n",
    "                line_thickness/3, \n",
    "                [255, 255, 255], \n",
    "                thickness = font_thickness,\n",
    "                lineType = cv2.LINE_AA)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5bacccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the IoU of two bounding boxes.\n",
    "# The Intersection over Union (IoU) if a measure of the overlap between two bounding boxes.\n",
    "# IoU is the Jaccard Similarity of the areas of two objects in a plane.\n",
    "# In computer vision it is used to correctly detect an object.\n",
    "# By convertion, the predicted bouding box is considered correct if the IoU is greater than 0.5.\n",
    "# Incrasing the threshold improves precision but worsens recall.\n",
    "# If the predicted bbox and the real bbox overlapped perfectly, the IoU would be 1.\n",
    "# Non-Maximal Supression (NMS) cleans the multiple detections and keeps only one detection per object. For this, it ...\n",
    "# ... chooses the highest probability bbox and supresses all other bboxes whose IoU is greater. Therefore, in the end ...\n",
    "# ... only one bbox is kept, likely the most precise one (and unlikely the least precise one).\n",
    "def bbox_iou(box1, box2):\n",
    "    \n",
    "    # Calculate the maximum and minimum intersection\n",
    "    inter_max_xy = torch.min(box1[:, 2:4], box2[:, 2:4])\n",
    "    inter_min_xy = torch.min(box1[:, 0:2], box2[:, 0:2])\n",
    "    \n",
    "    # Calculate the intersection area\n",
    "    inter_size = torch.clamp((inter_max_xy - inter_min_xy), min=0)\n",
    "    inter_area = inter_size[:, 0] * inter_size[:, 1]\n",
    "    \n",
    "    # Calculate the areas\n",
    "    b1_area = (box1[:, 2] - box1[:, 0])*(box1[:, 3] - box1[:, 1])\n",
    "    b2_area = (box2[:, 2] - box2[:, 0])*(box2[:, 3] - box2[:, 1])\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d41345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make the detections\n",
    "# To obtain true detections, subjecting the output to the objectiveness threshold and to the Non-Maximal Supression (NMS)\n",
    "# Return a tensor of shape: (D * 8), in which D in the number of true detections in all images, each represented by a line.\n",
    "# Each detection has the attributes: image index for the batch to which the image belongs, 4 bbox coordinates, ...\n",
    "# ... objectiveness score, score of the max confidence class, and class index.\n",
    "def write_results(prediction, confidence, num_classes, nms_conf=0.4):\n",
    "    \n",
    "    # Task 1: Objectness confidence thresholding\n",
    "    conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n",
    "    prediction *= conf_mask\n",
    "    \n",
    "    # Task 2: Locate the bbox corners\n",
    "    box_corner = prediction.detach().clone()\n",
    "    box_corner[:, :, 0] = (predictionbox_corner[:, :, 0] - box_corner[:, :, 2]/2)\n",
    "    box_corner[:, :, 1] = (predictionbox_corner[:, :, 1] - box_corner[:, :, 3]/2)\n",
    "    box_corner[:, :, 2] = (predictionbox_corner[:, :, 2] - box_corner[:, :, 2]/2)\n",
    "    box_corner[:, :, 3] = (predictionbox_corner[:, :, 3] - box_corner[:, :, 3]/2)\n",
    "    prediction[:, :, :4] = box_corner[:, :, :4]\n",
    "\n",
    "    batch_size = prediction.size(0)\n",
    "    write = False\n",
    "    \n",
    "    # Task 3: Loop through the batch's images\n",
    "    # Confidence threshold -> Only care about the value of the largest score class.\n",
    "    # Get the index of the largest score class and its score.\n",
    "    for ib in range(batch_size):\n",
    "        \n",
    "        # Image prediction\n",
    "        image_prediction = prediction[ib]\n",
    "        \n",
    "        # Indexes and ajust dimensions\n",
    "        max_conf, max_conf_indexes = torch.max(image_prediction[:, 5:5+num_classes], dim=1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_indexes = max_conf_indexes.float().unsqueeze(1)\n",
    "        image_prediction = torch.cat((image_prediction[:, :5], max_conf, max_conf_indexes), dim=1)\n",
    "        \n",
    "        # Get rid of the zero objectiveness lines\n",
    "        non_zero_indexes = torch.nonzero(image_prediction[:, 4]).squeeze()\n",
    "        image_prediction_ = image_prediction[non_zero_indexes, :].view(-1, 7)\n",
    "        \n",
    "        # If there is no prediction, go to next iteration\n",
    "        if image_prediction_.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get the classes detected in the image\n",
    "        img_classes = unique(img_prediction_[:, -1])\n",
    "        \n",
    "        # NMS of each class\n",
    "        for cls in img_classes:\n",
    "            \n",
    "            # Get the detections attributed to the current class\n",
    "            cls_mask = image_prediction_ * (image_prediction_[:, -1] == cls).float().unsqueeze(1)\n",
    "            cls_mask_indexes = torch.nonzero(cls_mask[:, -2]).squeeze()\n",
    "            \n",
    "            # Get the bboxes with the same class\n",
    "            img_pred_classes = image_prediction_[cls_mask_indexes].view(-1, 7)\n",
    "            \n",
    "            # Classificate the detections along the objectiveness score sequence from highest to lowest\n",
    "            obj_conf_desc_indices = torch.sort(img_pred_classes[:, 4], descending=True)[1]\n",
    "            img_pred_classes = img_pred_classes[obj_conf_desc_indices]\n",
    "            num_detections = img_pred_classes.size(0)\n",
    "            \n",
    "            # NMS\n",
    "            for i in range(num_detections):\n",
    "                \n",
    "                # Obtain the IoU (intersection over union) for all boxes below the one being seen\n",
    "                try:\n",
    "                    ious = bbox_iou(img_pred_classes[i].unsqueeze(0), img_pred_classes[i+1])\n",
    "                except ValueError:\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    break\n",
    "                \n",
    "                # Zero all detections with have a IoU > threshold, that is, similar to the above bbox\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                img_pred_classes[i+1:] *= iou_mask\n",
    "                \n",
    "                # Keep the non-zero lines, including bboxes distinct from the above bbox\n",
    "                non_zero_indexes = torch.nonzero(img_pred_classes[:, 4]).squeeze()\n",
    "                img_pred_classes = img_pred_classes[non_zero_indexes].view(-1, 7)\n",
    "                \n",
    "            # Saving the predictions\n",
    "            # For each batch with index i, with k detections, the batch_indexes will be a k-by-1 tensor filled with i\n",
    "            batch_indexes = img_pred_classes.new_full((img_pred_classes.size(0), 1), ib)\n",
    "            \n",
    "            # Generate the final tuple\n",
    "            seq = batch_indexes, img_pred_classes\n",
    "            \n",
    "            # Concatenate the detections\n",
    "            if write is False:\n",
    "                output = torch.cat(seq, dim=1)\n",
    "                write = True\n",
    "            else:\n",
    "                new_out = torch.cat(seq, dim=1)\n",
    "                output = torch.cat((output, new_out))\n",
    "                \n",
    "    \n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a18c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write the bounding box for each image\n",
    "def write_bbox(x, results, classes, colors):\n",
    "    \n",
    "    # bbox coordinates\n",
    "    corner1 = tuple(x[1:3].int())\n",
    "    corner2 = tuple(x[3:5].int())\n",
    "    \n",
    "    # Image\n",
    "    img = results[int(x[0])]\n",
    "    \n",
    "    # Bbox line thickness\n",
    "    line_thickness = round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    \n",
    "    # Class\n",
    "    cls = int(x[-1])\n",
    "    \n",
    "    # Color\n",
    "    color = random.choice(colors)\n",
    "    \n",
    "    # Label\n",
    "    label = f\"{classes[cls]}\"\n",
    "    \n",
    "    # Create the rectangle (bbox) in the image using OpenCV\n",
    "    cv2.rectangle(img, corner1, corner2, color, thickness=line_thickness)\n",
    "    \n",
    "    # Font thickness\n",
    "    font_thickness = max(line_thickness - 1, 1)\n",
    "    \n",
    "    # Font size\n",
    "    font_size = cv2.getTextSize(label, 0, fontScale=line_thickness/3, thickness=line_thickness)[0]\n",
    "    \n",
    "    # Corner\n",
    "    corner2 = corner1[0] + font_size[0]+3, corner1[1]-t_size[1]-3\n",
    "    \n",
    "    # Write bbox and text\n",
    "    cv2.rectangle(img, corner1, corner2, color, -1)\n",
    "    cv2.putText(img, \n",
    "                label, \n",
    "                (corner1[0], corner1[1]-2), \n",
    "                0, \n",
    "                line_thickness/3, \n",
    "                [255, 255, 255], \n",
    "                thickness=line_thickness,\n",
    "                lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8124cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Get user inputs\n",
    "    args = arg_parse()\n",
    "    \n",
    "    # Define the input data\n",
    "    images = args.input\n",
    "    \n",
    "    # Batch size\n",
    "    batch_size = args.batch\n",
    "    \n",
    "    # Prediction confidence\n",
    "    confidence = float(args.confidence)\n",
    "    \n",
    "    # Non-Maximal Suppression (NMS)\n",
    "    nms_thresh = float(args.nms_thresh)\n",
    "    \n",
    "    # Control variable\n",
    "    start = 0\n",
    "    \n",
    "    # Check if there is an available GPU\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    # Load the classes\n",
    "    num_classes = 80\n",
    "    classes_file = open('classes/coco.names', 'r')\n",
    "    classes_names = classes_file.read().split('\\n')[:-1]\n",
    "    classes = class_names\n",
    "    \n",
    "    # Load the YOLO model\n",
    "    print(\"\\nLoading Model...\")\n",
    "    model = Darknet(args.cfg_file)\n",
    "    model.load_weights(args.weights)\n",
    "    print(\"\\nModel Loaded Successfully!\")\n",
    "    \n",
    "    # Define the resolution of the input images\n",
    "    model.net_info['height'] = args.resolution\n",
    "    \n",
    "    # Input dimensions\n",
    "    input_dim = int(model.net_info['height'])\n",
    "    \n",
    "    # Return an error if the dimensions are inadequate\n",
    "    assert input_dim % 32 == 0\n",
    "    assert input_dim > 0\n",
    "    \n",
    "    # If there is a GPU, send the model to it\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "        \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if files and directories exist\n",
    "    try:\n",
    "        # Fetch the images for detection\n",
    "        img_list = [os.path.join(os.path.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "    except NotADirectoryError:\n",
    "        img_list = []\n",
    "        img_list.append(os.path.join(os.path.realpath('.'), images))\n",
    "    except FileNotFoundError:\n",
    "        print(f'Could not find a file named {images}')\n",
    "        exit()\n",
    "    \n",
    "    # If the output directory does not exist, creat it\n",
    "    if not os.path.exists(args.output):\n",
    "        os.makedirs(args.output)\n",
    "    \n",
    "    # Load the images, reading them with opencv\n",
    "    loaded_images = [cv2.imread(image) for image in img_list]\n",
    "    \n",
    "    # Define the batches\n",
    "    img_batches = list(map(prep_image, loaded_images, [input_dim for idx in range(len(img_list))]))\n",
    "    \n",
    "    # Input image dimensions\n",
    "    img_dim_list = [(img.shape[1], img.shape[0]) for img in loaded_images]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_dim_list = torch.FloatTensor(img_dim_list).repeat(1, 2)\n",
    "    \n",
    "    # Create the image batches and send them to the GPU, if one is available\n",
    "    # Each batch is a concatenation of images according to batch_size\n",
    "    \n",
    "    # Check if there is a single image\n",
    "    counter = 0\n",
    "    if(len(img_dim_list) % batch_size):\n",
    "        counter = 1\n",
    "    \n",
    "    # Handle image batching\n",
    "    if batch_size != 1:\n",
    "        num_batches = len(img_list) // batch_size + counter\n",
    "        img_batches = [torch.cat((img_batches[idx * batch_size : min((idx+1) * batch_size, len(img_batches))]), dim=0) \n",
    "                       for idx in range(num_batches)]\n",
    "    if CUDA:\n",
    "        img_dim_list = img_dim_list.cuda()\n",
    "    \n",
    "    # Control variable for the dectection loop\n",
    "    write = False\n",
    "    \n",
    "    # Checkpoint\n",
    "    start_det_loop = time.time()\n",
    "    \n",
    "    # Loop through the image batches and detect\n",
    "    for idx, batch in enumerate(img_batches):\n",
    "        \n",
    "        # Checkpoint\n",
    "        start = time.time()\n",
    "        \n",
    "        # If using GPU, send the batch there\n",
    "        if CUDA:\n",
    "            batch = batch.cuda()\n",
    "            \n",
    "        # Predict with the model\n",
    "        with torch.no_grad():\n",
    "            prediction = model(batch, CUDA)\n",
    "            \n",
    "        # Store the prediction details\n",
    "        prediction = write_results(prediction=prediction, confidence=confidence, num_classes=num_classes, nms_conf=nms_thresh)\n",
    "        \n",
    "        # Checkpoint\n",
    "        end = time.time()\n",
    "        \n",
    "        # If type(prediction) == int, that means there is no prediction for this batch\n",
    "        if type(prediction) == int:\n",
    "            for img_index, image in enumerate(img_list[idx*batch_size : min((idx+1)*batch_size, len(img_list))]):\n",
    "                \n",
    "                # Print the details about the detection attempt\n",
    "                print(f\"{image.split('/')[-1]:20s} predicted in {(end-start)/batch_size:6.3f} seconds.\")\n",
    "                print(f\"{'Deteted objects:':20s}\")\n",
    "                print(\"-----------------------------------------------\")\n",
    "                \n",
    "            continue\n",
    "        \n",
    "        # Bounding box indexes\n",
    "        prediction[:, 0] += idx*batch_size\n",
    "        \n",
    "        if write is False:\n",
    "            output = prediction\n",
    "            write = True\n",
    "        else:\n",
    "            output = torch.cat((output, prediction))\n",
    "            \n",
    "        # Loop through the bbox indexes\n",
    "        for img_idx, image in enumerate(img_list[idx*batch_size: min((idx+1)*batch_size, len(img_list))]):\n",
    "            \n",
    "            # Generate a global index\n",
    "            global_img_index = idx*batch_size + img_idx # Image index on all images\n",
    "            \n",
    "            # Get the classes according go the indexes\n",
    "            objs = [classes[int(x[-1])] for x in output if int(x[0]) == global_img_index]\n",
    "            \n",
    "            # Print the result\n",
    "            print(f\"{image.split('/')[-1]:20s} predicted in {(end-start)/batch_size:6.3f} seconds.\")\n",
    "            print(f\"{'Deteted objects:':20s} {' '.join(objs)}\")\n",
    "            print(\"-----------------------------------------------\")\n",
    "            \n",
    "        # Synchonize the GPU with the CPU\n",
    "        if CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        # Draw the bounding boxes\n",
    "        try:\n",
    "            output\n",
    "        except:\n",
    "            print(\"No detection was made!\")\n",
    "            exit()\n",
    "            \n",
    "        # Adjust the bbox coordinates for inserting in the image\n",
    "        \n",
    "        # Filter only the images with detections\n",
    "        img_dim_list = torch.index_select(img_dim_list, 0, output[:, 0].long())\n",
    "        \n",
    "        # Image scale factor\n",
    "        scaling_factors = torch.min(input_dim / img_dim_list, 1)[0].view(-1, 1)\n",
    "        \n",
    "        # x coordinate of the bbox corners\n",
    "        output[:, [1, 3]] -= (input_dim - scaling_factors * img_dim_list[:, 0].view(-1, 1)) / 2\n",
    "\n",
    "        # y coordinate of the bbox corners\n",
    "        output[:, [2, 4]] -= (input_dim - scaling_factors * img_dim_list[:, 1].view(-1, 1)) / 2\n",
    "        \n",
    "        # Redimension to the original size\n",
    "        output[:, 1:5] /= scaling_factors\n",
    "        \n",
    "        # Clip the bboxes whose limits are outside the image borders\n",
    "        # https://pytorch.org/docs/stable/generated/torch.clamp.html\n",
    "        for i in range(output.shape[0]):\n",
    "            output[i, [1, 3]] = torch.clamp(output[i, [1, 3]], 0.0, img_dim_list[i, 0])\n",
    "            output[i, [2, 4]] = torch.clamp(output[i, [2, 4]], 0.0, img_dim_list[i, 1])\n",
    "            \n",
    "        # Color palette\n",
    "        colors = pkl.load(open('palette', 'rb'))\n",
    "        \n",
    "        # Checkpoints\n",
    "        output_recast = time.time()\n",
    "        class_load = time.time()\n",
    "        draw_start = time.time()\n",
    "        \n",
    "        # Iteration\n",
    "        list(map(lambda x: write_bbox(x, loaded_images, classes, colors), output))\n",
    "        \n",
    "        # Checkpoints\n",
    "        draw_end = time.time()\n",
    "        \n",
    "        # Paths to save the images and detected objects\n",
    "        det_names = pd.Series(img_list).apply(lambda x: f\"{args.output}/det_{x.plit('/')[-1]}\")\n",
    "        \n",
    "        # Save images with detections to the paths in det_names\n",
    "        list(map(cv2.imwrite, det_names, loaded_images))\n",
    "        \n",
    "        print(\"Summary\")\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(f\"{'Task':25s}: {'Total Time (seconds)'}\")\n",
    "        print()\n",
    "        print(f\"{'Checking Directory':25s}: {load_batch - read_dir:2.3f}\")\n",
    "        print(f\"{'Loading Batch':25s}: {start_det_loop - load_batch:2.3f}\")\n",
    "        print(f\"{'Detecting ('+str(len(img_list))+' images':25s}: {output_recast - start_det_loop:2.3f}\")\n",
    "        print(f\"{'Processing Outputs':25s}: {class_load - output_recast:2.3f}\")\n",
    "        print(f\"{'Drawing Bounding Boxes':25s}: {draw_start - draw_end:2.3f}\")\n",
    "        print(f\"{'Mean Image Loading Time':25s}: {(end - load_batch) / len(img_list):2.3f}\")\n",
    "        print(\"------------------------------------------------\")\n",
    "        \n",
    "        # Clean the GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print('\\nDetection Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ed0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
